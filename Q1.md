# Q1: Big O Notation in Algorithm Analysis

Big O notation is defined as a branch of mathematics that concerns itself with discussing both the **time** and **space complexity** of an algorithm in an **upper-bound sense**. It helps derive a measure of efficiency as the input size approaches infinity.

---

## **Big O Notation Algorithm Analysis Guidelines**

### 1. **Ignore Constants**
   - **Explanation**: Constants and lower-order terms are ignored in Big O notation because they become insignificant for larger input sizes.
   - **Examples**:
     - `O(2n)` becomes `O(n)`.
     - `O(n + 5)` becomes `O(n)`.

---

### 2. **Concentrate on the Dominant Term**
   - **Explanation**: Only the term with the highest growth rate is considered in Big O notation.
   - **Example**:
     - `O(n^2 + n)` becomes `O(n^2)`.

---

### 3. **Iterative Loops**
   - **Explanation**:
     - A single loop iterating `n` times has a complexity of `O(n)`.
     - Nested loops multiply their complexities.
   - **Examples**:
     - One loop: `O(n)`.
     - Nested loops: `O(n^2)`.

---

### 4. **Successive Operations Add**
   - **Explanation**: If operations are executed in sequence (one after another), their complexities are added, and the dominant term is kept.
   - **Example**:
     - `O(n) + O(n^2)` becomes `O(n^2)`.

---

### 5. **Recursive Calls Multiply**
   - **Explanation**: Recurrence relations are used to analyze recursive algorithms. The complexity depends on how the input size changes with each recursive call.
   - **Example**:
     - Binary search splits the input in half each time: `O(log n)`.

---

### 6. **Logarithmic Growth**
   - **Explanation**: Logarithmic complexity is typical for algorithms that divide the problem into smaller parts (e.g., divide-and-conquer).
   - **Example**:
     - Binary search: `O(log n)`.

---

### 7. **Drop Non-Dominant Terms**
   - **Explanation**: Only the term with the highest growth rate is retained for combined complexities.
   - **Example**:
     - `O(n + n^2 + log n)` becomes `O(n^2)`.

---

### 8. **Constant Time Operations**
   - **Explanation**: Operations that take a constant amount of time are denoted as `O(1)`.
   - **Example**:
     - Accessing an array element: `arr[5]` â†’ `O(1)`.

---

### 9. **Tuple Multiplication in Nested Loops**
   - **Explanation**: If loops are nested and rely on the input size, their complexities are multiplied.
   - **Example**:
     - `O(n * n) = O(n^2)`.

---

### 10. **Amortized Complexity**
   - **Explanation**: Amortized analysis accounts for occasional expensive operations averaged over many inputs.
   - **Example**:
     - Resizing an array-backed list (e.g., dynamic arrays in Python): `O(1)` on average.

---

### 11. **Input Size Changes Matter**
   - **Explanation**: If the input size reduces (e.g., recursive halving), the complexity often involves logarithmic growth.
   - **Example**:
     - Merge Sort: `O(n log n)`.

---

### 12. **Space Complexity Rules**
   - **Explanation**: Space complexity accounts for extra memory usage, such as new arrays, recursion stacks, or auxiliary data structures.
   - **Examples**:
     - Recursive Fibonacci: `O(n)` space due to the recursion stack.
     - Merge Sort: `O(n)` space for auxiliary arrays.

---
